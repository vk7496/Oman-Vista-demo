import os
import time
import random
import requests
import streamlit as st
import folium
from streamlit_folium import st_folium

# ---------- Page Config & Styles ----------
st.set_page_config(page_title="OmanVista â€“ AI Tourism Explorer", page_icon="ğŸŒ´", layout="wide")
PRIMARY = "#006666"

st.markdown(f"""
<style>
.big-title {{ font-size: 34px; color: {PRIMARY}; text-align: center; font-weight: 800; margin: 6px 0 2px; }}
.sub-title {{ font-size: 15px; color: #333; text-align: center; margin: 0 0 20px; }}
.card-title {{ font-weight: 700; font-size: 18px; margin: 4px 0; color: #111; }}
.muted {{ color:#666; font-size: 12px; }}
.section {{ padding: 6px 0 2px; }}
</style>
""", unsafe_allow_html=True)

# ---------- Secrets / Keys ----------
OPENAI_KEY = st.secrets.get("OPENAI_API_KEY", "")
UNSPLASH_KEY = st.secrets.get("UNSPLASH_ACCESS_KEY", "7MjJqxvtlvgkayIdLZM69n4yFIol0J6cjxeRvciMznQ")

# ---------- Language Selector ----------
lang = st.sidebar.radio("ğŸŒ Language | Ø§Ù„Ù„ØºØ©", ["English", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"], index=0)

TXT = {
    "title": {"English": "OmanVista â€“ AI Tourism Explorer", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "Ø¹ÙÙ…Ø§Ù† ÙÙŠØ³ØªØ§ â€“ Ù…Ø³ØªÙƒØ´Ù Ø§Ù„Ø³ÙŠØ§Ø­Ø© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"},
    "subtitle": {
        "English": "Discover Omanâ€™s hidden gems with bilingual AI content.",
        "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "Ø§ÙƒØªØ´Ù Ø¬ÙˆØ§Ù‡Ø± Ø¹ÙÙ…Ø§Ù† Ø§Ù„Ù…Ø®ÙÙŠØ© Ø¨Ù…Ø­ØªÙˆÙ‰ Ø°ÙƒÙŠ Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„Ù„ØºØ©."
    },
    "filters": {"English": "Filters", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "Ø§Ù„Ù…Ø±Ø´Ø­Ø§Øª"},
    "region": {"English": "Region", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "Ø§Ù„Ù…Ù†Ø·Ù‚Ø©"},
    "category": {"English": "Category", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "Ø§Ù„ÙØ¦Ø©"},
    "search": {"English": "Search (name/tags)", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "Ø¨Ø­Ø« (Ø§Ø³Ù…/ÙˆØ³ÙˆÙ…)"},
    "results": {"English": "results", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "Ù†ØªÙŠØ¬Ø©"},
    "map": {"English": "Interactive Map", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "Ø®Ø±ÙŠØ·Ø© ØªÙØ§Ø¹Ù„ÙŠØ©"},
    "chat": {"English": "AI Travel Assistant", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø³ÙØ± Ø§Ù„Ø°ÙƒÙŠ"},
    "reddit": {"English": "Community Buzz (Reddit)", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "Ø£Ø­Ø¯Ø« Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø§Øª (Reddit)"},
    "view_map": {"English": "View on Map", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "Ø¹Ø±Ø¶ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø±ÙŠØ·Ø©"},
}

st.markdown(f"<div class='big-title'>ğŸŒ´ {TXT['title'][lang]} ğŸŒ´</div>", unsafe_allow_html=True)
st.markdown(f"<div class='sub-title'>{TXT['subtitle'][lang]}</div>", unsafe_allow_html=True)

# ---------- Seed Data (stable â€“ no external API needed) ----------
ATTRACTIONS = [
    {"slug":"sultan-qaboos-grand-mosque","name_en":"Sultan Qaboos Grand Mosque","name_ar":"Ø¬Ø§Ù…Ø¹ Ø§Ù„Ø³Ù„Ø·Ø§Ù† Ù‚Ø§Ø¨ÙˆØ³ Ø§Ù„Ø£ÙƒØ¨Ø±",
     "region":"Muscat","category":"Culture & Architecture","tags":["mosque","architecture","landmark"],
     "lat":23.5859,"lon":58.4078},
    {"slug":"mutrah-corniche","name_en":"Mutrah Corniche","name_ar":"ÙƒÙˆØ±Ù†ÙŠØ´ Ù…Ø·Ø±Ø­",
     "region":"Muscat","category":"City & Waterfront","tags":["sea","promenade","market","sunset"],
     "lat":23.6155,"lon":58.5638},
    {"slug":"nizwa-fort","name_en":"Nizwa Fort","name_ar":"Ù‚Ù„Ø¹Ø© Ù†Ø²ÙˆÙ‰",
     "region":"Ad Dakhiliyah","category":"Heritage & Fort","tags":["fort","history","culture","market"],
     "lat":22.9333,"lon":57.5333},
    {"slug":"jebel-shams","name_en":"Jebel Shams","name_ar":"Ø¬Ø¨Ù„ Ø´Ù…Ø³",
     "region":"Ad Dakhiliyah","category":"Mountains & Hiking","tags":["mountain","hiking","viewpoints","nature"],
     "lat":23.2386,"lon":57.2742},
    {"slug":"wadi-bani-khalid","name_en":"Wadi Bani Khalid","name_ar":"ÙˆØ§Ø¯ÙŠ Ø¨Ù†ÙŠ Ø®Ø§Ù„Ø¯",
     "region":"Ash Sharqiyah","category":"Wadi & Pools","tags":["wadi","swimming","oasis","hike"],
     "lat":22.6000,"lon":59.2000},
    {"slug":"wahiba-sands","name_en":"Wahiba Sands (Sharqiya Sands)","name_ar":"Ø±Ù…Ø§Ù„ Ø§Ù„Ø´Ø±Ù‚ÙŠØ©",
     "region":"Ash Sharqiyah","category":"Desert & Adventure","tags":["desert","dunes","camp","4x4"],
     "lat":21.4500,"lon":58.8000},
    {"slug":"ras-al-jinz","name_en":"Ras Al Jinz Turtle Reserve","name_ar":"Ù…Ø­Ù…ÙŠØ© Ø§Ù„Ø³Ù„Ø§Ø­Ù Ø¨Ø±Ø£Ø³ Ø§Ù„Ø¬Ù†Ø²",
     "region":"Ash Sharqiyah","category":"Wildlife & Nature","tags":["turtles","beach","wildlife","night"],
     "lat":22.3500,"lon":59.4500},
    {"slug":"mughsail-beach","name_en":"Al Mughsail Beach","name_ar":"Ø´Ø§Ø·Ø¦ Ø§Ù„Ù…ØºØ³ÙŠÙ„",
     "region":"Dhofar (Salalah)","category":"Beach & Blowholes","tags":["beach","blowholes","cliffs","khareef"],
     "lat":16.8820,"lon":53.7290},
]

# ---------- Unsplash Helper ----------
def fetch_unsplash_image(query: str, key: str, orientation: str = "landscape") -> str | None:
    if not key:
        return None
    try:
        url = "https://api.unsplash.com/search/photos"
        params = {"query": query, "per_page": 5, "orientation": orientation, "client_id": key}
        r = requests.get(url, params=params, timeout=12)
        r.raise_for_status()
        data = r.json()
        if data.get("results"):
            pick = random.choice(data["results"])
            return pick["urls"]["regular"]
        return None
    except Exception as e:
        st.warning(f"Unsplash error for '{query}': {e}")
        return None

def get_image_for(item) -> str:
    q = f"{item['name_en']} {item['region']} Oman"
    url = fetch_unsplash_image(q, UNSPLASH_KEY)
    if url:
        return url
    # fallback (placeholder always works)
    return f"https://placehold.co/1000x600?text={item['name_en'].replace(' ','%20')}"

# ---------- Filters ----------
st.sidebar.header(TXT["filters"][lang])
regions = ["All"] + sorted({a["region"] for a in ATTRACTIONS})
cats = ["All"] + sorted({a["category"] for a in ATTRACTIONS})
sel_region = st.sidebar.selectbox(TXT["region"][lang], regions, index=0)
sel_category = st.sidebar.selectbox(TXT["category"][lang], cats, index=0)
q = st.sidebar.text_input(TXT["search"][lang], "")

filtered = ATTRACTIONS
if sel_region != "All":
    filtered = [a for a in filtered if a["region"] == sel_region]
if sel_category != "All":
    filtered = [a for a in filtered if a["category"] == sel_category]
if q.strip():
    qq = q.strip().lower()
    filtered = [
        a for a in filtered
        if qq in a["name_en"].lower()
        or qq in a["name_ar"]
        or qq in a["region"].lower()
        or qq in a["category"].lower()
        or any(qq in t.lower() for t in a["tags"])
    ]

st.caption(f"Showing {len(filtered)} {TXT['results'][lang]}")

# ---------- Map ----------
st.markdown(f"### ğŸ—ºï¸ {TXT['map'][lang]}")
if filtered:
    center_lat = sum(a["lat"] for a in filtered) / len(filtered)
    center_lon = sum(a["lon"] for a in filtered) / len(filtered)
    fmap = folium.Map(location=[center_lat, center_lon], zoom_start=6)
    for a in filtered:
        popup = a["name_en"] if lang == "English" else a["name_ar"]
        folium.Marker([a["lat"], a["lon"]], popup=popup).add_to(fmap)
    st_folium(fmap, width=900, height=420)
else:
    st.info("No items match your filters." if lang == "English" else "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…Ø±Ø´Ø­Ø§Øª.")

# ---------- Cards (images + descriptions) ----------
st.markdown("<div class='section'></div>", unsafe_allow_html=True)
cols_per_row = 3
for i in range(0, len(filtered), cols_per_row):
    cols = st.columns(cols_per_row)
    for c, item in zip(cols, filtered[i:i+cols_per_row]):
        with c:
            img_url = get_image_for(item)
            st.image(img_url, use_container_width=True)
            title = item["name_en"] if lang == "English" else item["name_ar"]
            meta = f"{item['region']} â€¢ {item['category']}"
            if lang == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
                desc = f"{item['name_ar']} ÙŠÙ‚Ø¹ Ø¶Ù…Ù† ÙØ¦Ø© {item['category']} ÙÙŠ {item['region']}. ÙˆØ¬Ù‡Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„ØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ø£ØµÙŠÙ„Ø© ÙˆØ§Ù„Ù…Ù†Ø§Ø¸Ø± Ø§Ù„Ø®Ù„Ø§Ø¨Ø©."
            else:
                desc = f"{item['name_en']} is a {item['category'].lower()} in {item['region']}, Oman. Perfect for authentic experiences and scenic views."
            st.markdown(f"<div class='card-title'>{title}</div>", unsafe_allow_html=True)
            st.markdown(f"<div class='muted'>{meta}</div>", unsafe_allow_html=True)
            st.write(desc)
            maps_url = f"https://www.google.com/maps?q={item['lat']},{item['lon']}"
            st.link_button(TXT["view_map"][lang], maps_url, use_container_width=True)
            time.sleep(0.05)

st.write("---")

# ---------- OpenAI Chatbot ----------
st.markdown(f"### ğŸ¤– {TXT['chat'][lang]}")
user_q = st.text_input("Ask about Oman âœ¨ | Ø§Ø³Ø£Ù„ Ø¹Ù† Ø¹ÙÙ…Ø§Ù†")
if user_q:
    if not OPENAI_KEY:
        st.warning("Add OPENAI_API_KEY in Secrets to enable the AI assistant.")
    else:
        try:
            # OpenAI (new SDK)
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_KEY)
            system_prompt = (
                "You are an expert Oman tourism assistant. Answer concisely and helpfully. "
                "Use the user's language (English or Arabic). If user asks for Arabic, answer in Arabic."
            )
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_q}
            ]
            # You can switch model to gpt-4o or gpt-4o-mini depending on quota
            resp = client.chat.completions.create(model="gpt-4o-mini", messages=messages, temperature=0.4)
            answer = resp.choices[0].message.content
            st.success(answer)
        except Exception as e:
            st.error(f"OpenAI error: {e}")

st.write("---")

# ---------- Reddit (optional) ----------
st.markdown(f"### ğŸ”Š {TXT['reddit'][lang]}")
enable_reddit = st.toggle("Enable Reddit feed (r/oman, r/travel)" if lang=="English" else "ØªÙØ¹ÙŠÙ„ Ø®Ù„Ø§ØµØ© Reddit")
topic = st.text_input("Reddit topic (e.g., Oman travel, Salalah)" if lang=="English" else "Ù…ÙˆØ¶ÙˆØ¹ Reddit (Ù…Ø«Ø§Ù„: Ø³ÙŠØ§Ø­Ø© Ø¹ÙÙ…Ø§Ù†)","Oman travel")

def fetch_reddit_posts(q: str, subreddits=("oman","travel"), limit=5):
    posts = []
    headers = {"User-Agent": "OmanVistaDemo/1.0"}
    for sub in subreddits:
        url = f"https://www.reddit.com/r/{sub}/search.json"
        params = {"q": q, "restrict_sr": 1, "sort": "new", "limit": limit}
        try:
            r = requests.get(url, params=params, headers=headers, timeout=10)
            r.raise_for_status()
            data = r.json()
            for c in data.get("data", {}).get("children", []):
                d = c.get("data", {})
                posts.append({"title": d.get("title","(no title)"), "url": "https://www.reddit.com"+d.get("permalink","")})
        except Exception:
            # fail silently to keep demo smooth
            pass
    return posts[:limit]

if enable_reddit:
    posts = fetch_reddit_posts(topic)
    if not posts:
        st.info("No recent Reddit posts found (or blocked by rate limits).")
    else:
        for p in posts:
            st.markdown(f"â€¢ [{p['title']}]({p['url']})")

st.markdown("<br><div style='text-align:center;color:#777;font-size:12px'>Â© Golden Bird â€¢ OmanVista (Demo)</div>", unsafe_allow_html=True)
